---
layout: post
title: DDPM Memo
subtitle: DDPM Memo
date: 2024-10-01 10:22:00 +0900
category: content
tags:
  - vision
use_math: true
---

Diffusion 모델의 근간이라고 할 수 있는 Denoising Diffusion Probabilistic Models 논문을 읽고, 메모한 내용을 기록합니다.

### 1. Introduction

A diffusion probabilistic model (which we will call a “diffusion model” for brevity) is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time.

- Diffusion Probablistic model
	-  Varational inference으로 유한 시간 후의 데이터와 매칭되는 샘플을 생성하여 학습된 파라미터화 된 마르코프 체인
	- anneald Langevin dynamics 를 multiple noise levels 로 학습한 denoising score matching 과 동일. Section 3.2
	- 경쟁력 있는 log likelhood는 가지지 못함. 그러나 에너지 기반 모델과 스코어 매칭 보다는 낫다. 
	- 데이터 압축이나 효율성을 고려할 때, 불필요한 정보에 과도한 공간을 할당하고 있다는 점을 지적하고 있습니다.

### 2. Background

ELBO의 적용 (수식 3 설명)

$$
\begin{aligned}
log\, p_\theta(x_0) &=  log \int p_\theta (x_{0:T}) dx_{0:T}
\\ &= log \int \frac{ q(x_{1:T} \vert x_0) p_\theta (x_{0:T})}{q(x_{1:T} \vert x_0)} dx_{0:T}
\\ &= log E_{q(x_{1:T} \vert x_0)} \left[ \frac{p_\theta (x_{0:T})}{q(x_{1:T} \vert x_0)} \right]
\\ & \ge  E_{q(x_{1:T} \vert x_0)} \left[ log \frac{p_\theta (x_{0:T})}{q(x_{1:T} \vert x_0)} \right]\text{ by Jensen's Inequality}
\\ & = E_{q(x_{1:T} \vert x_0)}  \left[ log\, \frac
{p(x_T) \prod_{t\ge 1} p_\theta(x_{t-1} \vert x_t)}
{\prod_{t\ge 1} q(x_t \vert x_{t-1})} 
\right]
\\ & = E_{q(x_{1:T} \vert x_0)}  \left[ log\, p(x_T)  + \sum_{t \ge 1} log \, \frac
{p_\theta(x_{t-1} \vert x_t)}
{ q(x_t \vert x_{t-1})} 
\right] =: L
\end{aligned}
$$

수식 5 설명 (순서만 다름)

$$
\begin{aligned}
log\,p(x) &\ge \mathbb{E}_{q(x_{1:T}\vert x_0)} \left[log\,\frac{p(x_{0:T})}{q(x_{1:T}\vert x_0)} \right]
\\&= \underbrace{\mathbb{E}_{q(x_1\vert x_0)} \left[log\, p_\theta(x_0 \vert x_1) \right]}_{\text{reconstruction term}} -
\underbrace{D_{KL}(q(x_T\vert x_0) \Vert p(x_T))}_{\text{prior matching term}} 
\\ & \quad \quad-
\sum^T_{t=2}
\underbrace{\mathbb{E}_{q(x_t \vert x_0)}\left[D_{KL}(q(x_{t-1}\vert x_t, x_0) \Vert p_\theta(x_{t-1}\vert x_t)) \right]}_{\text{denoising matching term}}
\end{aligned}
$$
Denoising matching term에 집중

수식 6 설명 (빡세다 ㅋㅋ)

$$
\begin{aligned}
q(x_{t-1} \vert x_t, x_0) &= \frac{q(x_t \vert x_{t-1}, x_0) q(x_{t-1} \vert x_0)}{ q(x_t \vert x_0)} \text{ (by Bayes rule)}
\\&= \frac
{N(x_t; \sqrt{\alpha_t}x_{t-1}, (1-\alpha_t)I) \, N\,(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}}\, x_0, \sqrt{1 - \bar{\alpha}_{t-1}}\, I)}
{N\,(x_t; \sqrt{\bar{\alpha}_t}\, x_0, \sqrt{1 - \bar{\alpha}_t}\, I)}
\\& \propto exp \left\{ 
-\left[
\frac{(x_t - \sqrt{\alpha_t} x_{t-1})^2}{2(1-\alpha_t)}
+ \frac{(x_{t-1} - \sqrt{\bar{\alpha}_{t-1}} x_0)^2}{2(1-\bar{\alpha}_{t-1})}
- \frac{(x_{t} - \sqrt{\bar{\alpha}_{t}} x_0)^2}{2(1-\bar{\alpha}_{t})}
\right]
\right\}
\\& = exp \left\{ 
- \frac 1 2 \left[
\frac{(-2\sqrt{\alpha_t} x_t x_{t-1} + \alpha_t x^2_{t-1}) }{1-\alpha_t}
+ \frac{(x^2_{t-1} - 2\sqrt{\bar{\alpha}_{t-1}}x_{t-1}x_0)}{1-\bar{\alpha}_{t-1}}
+ C(x_t, x_0)
\right]
\right\}
\\&\,(\,q(x_{t-1} \vert x_t, x_0)\text{는 주어진} x_t\text{와 } x_0 \text{에 대하여,  }x_{t-1} \text{을 계산 하므로, } x_t, x_0 \text{만 포함된 항은 상수 항으로 간주} )
\\& \propto exp \left\{ 
- \frac 1 2 \left[
- \frac{2\sqrt{\alpha_t} x_t x_{t-1} }{1-\alpha_t}
+ \frac{\alpha_t x^2_{t-1}}{1-\alpha_t}
+ \frac{x^2_{t-1}}{1-\bar{\alpha}_{t-1}}
- \frac{2\sqrt{\bar{\alpha}_{t-1}}x_{t-1}x_0}{1-\bar{\alpha}_{t-1}}
\right]
\right\}

\\& = exp \left\{ 
- \frac 1 2 \left(\frac{1-\bar{\alpha}_t}{(1-\alpha_t)(1-\bar{\alpha}_{t-1})} \right) \left[
 x^2_{t-1}
- 2 \frac{ 
\sqrt{\alpha_t} (1-\bar{\alpha}_{t-1}) x_t + \sqrt{\bar{\alpha}_{t-1}} (1-\alpha_t) x_0}
{\left(1-\bar{\alpha}_t \right)} x_{t-1}
\right]
\right\}
\\& \propto exp \left\{ 
- \frac 1 2 \left(\frac{1}{\frac{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}} \right) \left[\left(
 x_{t-1}
- \frac{ 
\sqrt{\alpha_t} (1-\bar{\alpha}_{t-1}) x_t + \sqrt{\bar{\alpha}_{t-1}} (1-\alpha_t) x_0}
{\left(1-\bar{\alpha}_t \right)}
\right)^2\right]
\right\}
\\&= N(x_{t-1}; \underbrace{\frac{ 
\sqrt{\alpha_t} (1-\bar{\alpha}_{t-1}) x_t + \sqrt{\bar{\alpha}_{t-1}} (1-\alpha_t) x_0}
{\left(1-\bar{\alpha}_t \right)}}_{\mu_q(x_t, x_0)}, \underbrace{\frac{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} I}_{\Sigma_q(t)}) \quad \text{( by 정규분포 p.d.f)}
\end{aligned}
$$

여기서 중요한 부분은 결국 $q(x_{t-1} \vert x_t, x_0)$ 를 정규 분포로 근사 한 것이다.
**즉, $x_{t-1} \sim q(x_{t-1}\vert x_t, x_0)$ 이 정규분포를 따르고, 그 평균이 $x_t, x_0$로 구성된 함수 $\mu(x_t, x_q)$ 이고, 분산이 $\alpha$로 구성된 $\Sigma_q(t)$ 임을 알 수 있다.**

$$
q(x_{t-1} \vert x_t, x_0) = N(x_{t-1}; \underbrace{\frac{ 
\sqrt{\alpha_t} (1-\bar{\alpha}_{t-1}) x_t + \sqrt{\bar{\alpha}_{t-1}} (1-\alpha_t) x_0}
{\left(1-\bar{\alpha}_t \right)}}_{\mu_q(x_t, x_0)}, \underbrace{\frac{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t} I}_{\Sigma_q(t)})
$$
### 3. Diffusion models and denoising autoencoders

#### 3.2 Reverse process and $L_{1:T-1}$(denoising matching terms)
수식 8에 대한 정리
$$
\begin{aligned}
&\underset{\theta}{argmin}\,D_{KL}\,(q(x_{t-1}\vert x_t, x_0) \Vert p_\theta(x_{t-1}\vert x_t))
\\ &= \underset{\theta}{argmin}\,D_{KL} (N(x_{t-1}; \mu_q, \Sigma_q(t)) \Vert N(x_{t-1};\mu_\theta, \Sigma_q(t)))
\\ &= \underset{\theta}{argmin}\, \frac 1 2 \left[log\,\frac{\vert \Sigma_q(t) \vert}{\vert \Sigma_q(t) \vert} - d + tr(\Sigma_q(t)^{-1}\Sigma_q(t)) + (\mu_\theta - \mu_q)^T\Sigma_q(t)^{-1}(\mu_\theta - \mu_q) \right]
\\ &= \underset{\theta}{argmin}\, \frac 1 2 \left[log\,1 - d + d + (\mu_\theta - \mu_q)^T\Sigma_q(t)^{-1}(\mu_\theta - \mu_q) \right]
\\ &= \underset{\theta}{argmin}\, \frac 1 2 \left[(\mu_\theta - \mu_q)^T\Sigma_q(t)^{-1}(\mu_\theta - \mu_q) \right]
\\ &= \underset{\theta}{argmin}\, \frac 1 2 \left[(\mu_\theta - \mu_q)^T{(\sigma^2_q(t)I)}^{-1}(\mu_\theta - \mu_q) \right]
\\ &= \underset{\theta}{argmin}\, \frac {1} {2\sigma^2_q(t)} \left[(\mu_\theta - \mu_q)^T(\mu_\theta - \mu_q) \right]
\\ &= \underset{\theta}{argmin}\, \frac {1} {2\sigma^2_q(t)} \left[\Vert \mu_\theta - \mu_q \Vert^2_2\right]
\end{aligned}
$$

수식 12에 대한 정리 (수식8과 수식 11을 이용)

$$
\begin{aligned}
&\underset{\theta}{argmin}\,D_{KL}\,(q(x_{t-1}\vert x_t, x_0) \Vert p_\theta(x_{t-1}\vert x_t))
\\ &= \underset{\theta}{argmin}\,D_{KL} (N(x_{t-1}; \mu_q, \Sigma_q(t)) \Vert N(x_{t-1};\mu_\theta, \Sigma_q(t)))
\\ &= \underset{\theta}{argmin}\, \frac {1} {2\sigma^2_q(t)} \left[\Vert \mu_\theta - \mu_q \Vert^2_2\right]
\\ &= \underset{\theta}{argmin}\, \frac {1} {2\sigma^2_q(t)} 
\left[ \left\Vert
\frac{1}{\sqrt{\alpha_t}}  x_t - \frac{1-\alpha_t}{\sqrt{1- \bar{\alpha}_t}\sqrt{\alpha_t}} \epsilon_0
- \frac{1}{\sqrt{\alpha_t}}  x_t + \frac{1-\alpha_t}{\sqrt{1- \bar{\alpha}_t}\sqrt{\alpha_t}} \hat{\epsilon}_\theta(x_t, t)
\right\Vert^2_2 \right]
\\ &= \underset{\theta}{argmin}\, \frac {1} {2\sigma^2_q(t)}  \frac{(1-\alpha_t)^2}{(1- \bar{\alpha}_t)\alpha_t}
\left[ \left\Vert
  \epsilon_0 - \hat{\epsilon}_\theta(x_t, t)
\right\Vert^2_2 \right]
\end{aligned}
$$
- 이게 Langevin-like reverse process 와 유사

참고. Score based model 의 Denoising Score Matching

$$
\begin{aligned}
&\frac 1 2 E_{q_\sigma{\tilde{x}}} \left[ \Vert \nabla_\tilde{x} log\, q_\sigma(\tilde{x}) - s_\theta(\tilde{x}) \Vert^2_2 \right] \,\text{: Score matching}
\\=& \frac 1 2 E_{p(x)} E_{q_\sigma(\tilde{x}\vert x)} \left[ \Vert \frac{1}{\sigma^2}(x - \tilde{x}) - s_\theta(\tilde{x}) \Vert^2_2 \right] \, \text{: denoising}
\end{aligned}
$$

$s_\theta(\tilde{x})$ 가 데이터에 추가된 노이즈 $(x - \tilde{x})$ 를 추정한다. (denoising)

### Key takeaways
- Training objective 단순화 (L simple)
	- Prior matching term 삭제 ($\beta$ 를 학습하지 않고, 고정 된 형태로 구성 하여 학습 효율 높임)
	- Denoising ($p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$) 을 가우시안으로 예측 할 때 그 분산을 학습하지 않고 시간에 종속되는 상수로 사용 $(\sigma^2_t \mathbf{I} )$ . 학습 효율 높임
- mean 대신 Reparmeterize 하여, $\epsilon$ 을 예측 하게하여, 잔차(Residual)를 학습 하는 방향으로 학습 성능을 높임. 

#### 메모
- 4.3 절의 Lossless codelength :
	- $x_0$ 에 대한 $- log p(x_0)$ 를 추정하는데 변분 경계가 사용됨
	- 변분 경계가 곧 Loss (KL Divergence 포함)
	- KL 발산이 모델의 예측분포와 실제 데이터 분포간의 차 이므로, 이 값이 적을 수록 더 적은 비트로 데이터를 표현.
	- 따라서 계산된 Loss 값이 Lossless codelength와 직접적으로 연관
- 4.3절은 Diffusion 모델이 효율적인 압축 및 복원 방식으로 활용 될 수 있음을 시사. 
