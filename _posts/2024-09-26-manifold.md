---
layout: post
title: Manifold learning ML
subtitle: Manifold learning ML
date: 2024-09-26 23:19:00 +0900
category: content
tags:
  - vision
use_math: true
---

CS485 ML for vision 과목을 듣고 정리 한 내용이다.
Eigenface를 통해 Manifold learning의 기초를 다룬다.

### EIGENFACE approach
- 얼굴 이미지에 포함된 정보를 추출
- 얼굴 이미지를 feature space로 project. 해당 **Feature space** (low dimension sub-space)는 얼굴 이미지간 중요한 분포를 Span 함.
- 그 주요한 피쳐들을 **eigenfaces** 로 알려져있고, 이는 얼굴 셋의 eigen vectors 또는 principal components 이다.(눈, 코,입 등에 대응되는 것이 아님)
- 새로운 이미지를 eigenfaces(**face space**)로 span된 subspace 에 projection 하여 Recognition을 수행한다. **face space** 에서의 위치를 비교하여 얼굴을 구분함.
- 얼굴 이미지는 Eigenface features 들의 weighted sum 으로 재생성(reconstruction) 될 수 있다.

### Face space
- Face space 는 얼굴 분포의 주성분($\mathbf{u_1, u_2, ..., u_M}$)으로 span 된다. 
- 또는 얼굴 이미지 셋의 Covariance matrix의 eigen vector로 span 된다.

### PCA for estimating the subspace
- 주어진 데이터셋 ${x_n}, n=1, ..., N \text{ and } x_n \in R^D$ 에 대하여, 우리의 목적은 D 보다 훨씬 작은 M차원의 공간으로 project 하는 것이다. 이때에 project 된 데이터의 분산은 최대화 되어야 한다. (압축하더라도 분산이 커야 구분점이 남고, 비교가 가능해짐.)
- 이러한 M 차원의 subspace는 M개의 eigenvectors $\mathbf{u_1, u_2, ..., u_M}$으로 부터 span 된다.
- 이 M개의 eigenvectors는 데이터의 Covariance matrix S 로부터 나오며 그중 M 개의 가장 큰 eigenvalues를 선택한다.
- 이 Eigenvectors $\mathbf{u_1, u_2, ..., u_M}$ 는 얼굴 이미지간 분산의 특성을 Feature화 한 것으로 간주 할 수 있다.

### Projection onto the Eigenfaces
- Face image $\mathbf{x}$ 를 eigenface coordinates로 변환하면 (projection coefficients or features) 아래와 같다.

$$
\begin{gather}
\mathbf{x} \rightarrow \left( \mathbf{(x - \bar{x})^T u_1, (x - \bar{x})^T u_2, ... , (x - \bar{x})^T u_M} \right) \text{ where } \mathbf{\bar{x}} \text{ is the average face}
\\  \therefore \mathbf{x \approx \bar{x} + a_1 u_1 + a_2 u_2 + ... + a_M u_M}
\end{gather}
$$

- Best M eigenfaces 로 부터 span 된 M차원의 subspace를 모든 가능한 이미지의 **face space** 라고 부름

### Procedures
- Step1. 학습용 얼굴 이미지를 정규화 함(Scale, orientation, translation e.g. using eye locations)
- Step2. 모든 이미지 $\mathbf{I}_n$ 을 벡터 $\mathbf{x}_n$ 으로 변환한다. 여기서 D = WH
- Step3. 평균 얼굴 벡터 $\mathbf{\bar{x}}$ 를 계산한다. 

$$
\mathbf{\bar{x}} = \frac 1 N \sum^N_{n=1} \mathbf{x}_n
$$

- Step4. 평균 얼굴을 이미지 벡터에 뺀다.

$$
\mathbf{\phi}_n = \mathbf{x}_n - \mathbf{\bar{x}}
$$

- Step5. Covariance matrix S 를 구한다. (데이터의 분산과 상관 관계를 분석하기 위함)

$$
\begin{gather}
\mathbf{S} = \frac 1 N \sum^N_{n=1} (\mathbf{x}_n - \mathbf{\bar{x}}) (\mathbf{x}_n - \mathbf{\bar{x}})^T = \frac 1 N \sum^N_{n=1} \mathbf{\phi}_n {\mathbf{\phi}_n}^T = \frac 1 N \mathbf{A A^T} \in \mathbf{R}^{D \times D}
\\ \mathbf{A} = \left[ \phi_1, \phi2, ... \phi_N \right] \in \mathbf{R}^{D \times N}
\end{gather}
$$

- Step6. S($\frac 1 N \mathbf{AA}^T$) 에 대하여 eigen vectors를 계산하기
	- $\mathbf{S u_i} = \lambda_i \mathbf{u}_i$
	- 만약 S는 full-rank matrix가 Full rank 라면, 여기서 i는 1부터 D 까지.
- Step7. Best M eigenvalues에 대응하는 M개의 eigenvectors 구성 ($\mathbf{u_1, u_2, ... , u_M}$)
	- 여기서 eigenvalues 는 covariance와 연관됨.
- Step8. 노말화된 $\phi_n$을 projection 하여 표현

$$
\begin{gather}
\omega_n = \left[ a_{n1}, a_{n2}, ... , a_{nM} \right]
\\ \text{where } a_{ni} = \phi^T_n \mathbf{u}_i \text{ , } i=1,...,M
\end{gather}
$$

- Step9(optional). 트레이닝 셋의 각 얼굴은 M eigenvectors의 조합으로 재 구성 될 수 있음

$$
\mathbf{\tilde{x}}_n = \mathbf{\bar{x}} + \sum^M_{i=1} a_{ni} \mathbf{u}_i
$$

- 여기서 M이 커지면 커질수록 재 구성된 얼굴 이미지가 원본에 가까워짐

### Testing
- Step1. $\mathbf{x}$ 정규화 : $\phi = \mathbf{x - \bar{x}}$
- Step2. eigenspace로 projection : $a_i = \phi^T \mathbf{u}_i  \, , \, i=1, ... , M$
- Step3. projection을 vector로 표시 : $\omega = [a_1, a_2, ... , a_M ]^T$
- Step4. 가장 가까운 요소 찾기(Nearest Neighbor) : $e = min_n \Vert \omega - \omega_n \Vert, \, n= 1, ..., N$
- Step5. 가장 가까운 요소로 트레이닝 셋 중 어떤 얼굴인지 구분 가능.



