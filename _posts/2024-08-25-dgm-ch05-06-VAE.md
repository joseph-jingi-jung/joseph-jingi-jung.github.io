---
layout: post
title: DGM(CS236) Lec05~06. Latent variable models
subtitle: Latent variable models(VAE)
date: 2024-08-25 18:15:00 +0900
category: content
tags:
  - vision
use_math: true
---

아래 내용은 Stanford의 CS236(Deep Generative Model) 강의를 듣고 필기한 노트입니다.

### Mixture models

- 여러 간단한 모델들을 합쳐, 하나의 복잡한 모델을 표현하자.
  $p(x)=\sum_z\,p(x,z)=\sum_z\,p(z)p(x \vert z)=\sum_{k=1}^Kp(z=k)N(x;\mu_k,\Sigma_k)$
- 이 수식에서 p(z)는 Categorial distribution.
- 뒤의 Normal 분포는 pre-defined된 look up table에서 선택 한다고 볼 수 있다.
- 이 이미지에서 주황색이 true distribution $p(x)$ 이고, 다른 Normal 분포 3개가 $p(z=k)$ 일 때의 Normal 분포 이다.

### Variational Autoencoder

- A mixture of an infinite number of Gaussians.
- 무수히 많은 Gaussian 분포를 조합하여 모델을 표현.
- $z$를 Gaussian에서 뽑고, 그 $z$를 Neural Network에 통과시켜서 $\mu, \sigma$ 를 구함.
  - $z\sim N(0,I)$
  - $p(x\vert z) = N(\mu_\theta(z), \Sigma_\theta(z))$ where $\mu_\theta(z), \Sigma_\theta(z)$ are neural networks
    - $N(\mu_\theta(z), \Sigma_\theta(z))$ 의 차원은 $x$의 차원에 맞춰짐.
  - $p(x\vert z)$는 간단한 모델이나, $z$에 대해 marginal 인 $p(x)$는 매우 복잡함

### Variational autoencoder with joint distribution

- 만약 모델이 Joint distribution임을 가정한다면, 관찰된 training data point $\bar{x}$ 의 확률 $p(X=\bar{x};\theta)$를 구하려면,
  \begin{aligned}
  \int_zp(X=\bar{x}, Z = z; \theta)dz = \int_zp(\bar{x},z;\theta)dz
  \end{aligned}

- 관찰된 Variable $X$에 대한 Dataset $D$와 관찰되지 않은 Variable $Z$가 주어짐.
- 이를 Maximum likelihood learning 을 한다면, $p(x) = \sum_zp(x,z)$ 이므로,
  \begin{aligned}
  log\prod_{x \in D}p(x;\theta) = \sum_{x \in D}log\,p(x;\theta) = \sum_{x \in D}log\, \sum_zp(x,z;\theta)
  \end{aligned}
- 여기서 $log\sum_zp(x,z;\theta)$는 다루기 힘들다.
- z를 30개의 바이너리 latent feature로 가정한다면, $2^{30}$ 개의 더하기 term으로 이루어짐.
- 따라서 **Approximation**이 필요함

### First attempt: Naive Monte Carlo

- Navie Monte Carlo를 이용하여, z를 uniform하게 뽑고, 기댓값을 sample average를 통해서 추정.
\begin{aligned}
P_\theta(x) = \sum_zp_\theta(x,z)=\vert Z \vert\sum_{z \in Z}\frac{1}{\vert Z \vert}p_\theta(x,z) = \vert Z\vert E_{z\sim Uniform(Z)}[P_\theta(x,z)]
\end{aligned}
- Sampling을 통해서 계산 가능하게 하였으나, 여전히 Not in practice.
- 대부분의 z에 대하여, $P_\theta(x,z)$는 매우 작음. (유니폼하게 뽑았기 때문에 대부분의 구성결과가 의미가 없음.)
- 더 나은 방법으로 z를 sampling 해야 함.

### Second attempt: Importance Sampling

- $z$를 임의의 분포 $q$ 에서 뽑고, 이를 이용해서 기댓값을 추정해보자.
\begin{aligned}
 P_\theta(x) = \sum_zp_\theta(x,z)=\sum_{z \in Z}\frac{q(z)}{q(z)}p_\theta(x,z)=\sum_{z \in Z}q(z)\frac{p_\theta(x,z)}{q(z)} = E_{z\sim q(Z)}[\frac{P_\theta(x,z)}{q(z)}]
\end{aligned}
- Monte Carlo 를 적용해본다면
  - Sample $z^{(1)},...,z^{(k)}$ from $q(z)$
  - Approximate expectation with sample average 
  \begin{aligned}
  p_\theta (x) \approx \frac{1}{k}\sum_{j=1}^k\frac{p_\theta(x,z^{(j)})}{q(z^{(j)})}
  \end{aligned}
  - 그럼 $q(z)$를 어떻게 선정해야할까?
  - 직관적, 주어진 모델 $P_\theta(x,z)$하에 $x$가 주어졌을 때 가능성이 높은 z를 자주 샘플링 하는 것.
- 트레이닝을 위해 log-likelihood $log(p_\theta(x))$ 가 필요.
- 따라서, 
\begin{aligned}
log(P_\theta(x)) = log(\sum_zp_\theta(x,z))=log(\sum_{z \in Z}\frac{q(z)}{q(z)}p_\theta(x,z))=log(E_{z\sim q(Z)}[\frac{P_\theta(x,z)}{q(z)}])
\end{aligned}
- $q(z)$를 넣는다 해도, 여전히 계산하기가 어려움.
- 잠재변수 $z$는 고차원 공간에 위치 할 수 있으며, $q(z)$는 데이터로부터 직접 관찰 할 수 있는 것이 아니기 때문.
- 이에 아이디어로 **Jensen Inequality** 를 적용해봄.
  - Jensen Inequality (for concave functions) - $log(px+(1-p)x') \geq plog(x) + (1-p)log(x')$

$$
\begin{aligned}
log(E_{z \sim q(z)}[f(z)]) &= log(\sum_zq(z)f(z)) 
\\ & \geq \sum_zq(z)\,log\,f(z)
\\ &= E_{z_q(z)}[log\,f(z)]
\\ & \text{Choosing } f(z)=\frac{p_\theta(x,z)}{q(z)}
\end{aligned}
$$

$$
\begin{aligned}
log(E_{z\sim q(Z)}[\frac{P_\theta(x,z)}{q(z)}]) \geq E_{z \sim q(z)}[log(\frac{p_\theta(x,z)}{q(z)})]
\end{aligned}
$$

- 이 식을 ELBO(Evidence lower bound)라고 부름.
  - **ELBO(Evidence lower bound)** 는 어떤 q에 대해서도 유효하다고 할 수 있으며, 다시 써보면,

$$
\begin{aligned}
&log(P_\theta(x)) \geq \sum_z q(z) log (\frac{P_\theta (x, z)}{q(z)})
\\ &= \sum_z q(z)log\,P_\theta(x,z) - \sum_z q(z)log\,q(z)
\\ &= \sum_z q(z)log\,P_\theta(x,z) + H(q)
\end{aligned}
$$

> (두번째 텀은 Entropy $H(q)$ of $q$)
    
- 여기서 $q(z)=P_\theta(z\vert x;\theta)$ 라고 가정한다면, Lower bound는 아래와 같이 변경된다

$$
\begin{aligned}
\sum_z p_\theta(z\vert x; \theta) log \frac{P_\theta(x,z; \theta)}{P_\theta(z\vert x; \theta)} &= \sum_z P_\theta(z\vert x; \theta)log \frac{P_\theta(z\vert x; \theta)P_\theta(x;\theta)}{P_\theta(z\vert x; \theta)}
\\ &= \sum_z P_\theta(z\vert x; \theta)log\, P_\theta(x;\theta)
\\ &= log\, (P_\theta(x; \theta))
\end{aligned}
$$

> 참고, ($\sum_z P_\theta(z\vert x; \theta) = 1$) 이므로

- 따라서, bound가 equal로 tight하게 바뀜을 알 수 있다.
- 또한 $ELBO(q)$를 KL-Divergence 관점에서 보려면,

$$
\begin{aligned}
ELBO(q) &= E_{z \sim q(z)}[log\frac{P_\theta(x,z)}{q(z)}]\,...\,\text{베이즈 정리}
\\ &= E_{z \sim q(z)}[log\frac{P_\theta(x)P_\theta(z\vert x)}{q(z)}]\,...\,\text{로그 성질로 분리}
\\ &= E_{z \sim q(z)}[log\,{P_\theta(x)}] + E_{z \sim q(z)}[log\frac{P_\theta(z|z)}{q(z)}]\,...\, log\,p(x)\,\text{는} z \text{와 무관한 상수}
\\ &= log\,P_\theta(x)+ E_{z \sim q(z)}[log\frac{P_\theta(z\vert x)}{q(z)}]
\\ & q(z)와\,P_\theta(z\vert x)\,\text{사이의 KL-divergence는 }E_{z \sim q(z)}[log\frac{q(z)}{P_\theta(z\vert x)}]
\\ &\text{따라서, } D_{KL}(q(z) \Vert {P_\theta(z\vert x)}) = E_{z \sim q(z)}[log\frac{q(z)}{P_\theta(z \vert x)}]\text{ 이고, ELBO 수식을 다시 정리하면,}
\\ &log\,P_\theta(x) - D_{KL}(q(z) \Vert {P_\theta(z \vert x)})
\end{aligned}
$$

- 따라서, 아래와 같이 정리 할 수 있다. 

$$
log\,p(x; \theta) = ELBO + D_{KL}(q(z)\Vert p(z\vert x; \theta))
$$

- **즉 $q(z)$가 $p(z \vert x;\theta)$에 가까워 질 수록, ELBO는 true log-likelihood에 가까워 진다고 할 수 있다.**

### Interactable Posteriors

- 여기서 $p(z \vert x; \theta)$가 intractable 하다면? (EM에서는 가능한 경우를 다루나, 대부분의 경우는 불가능)
- VAE에서는 decoder (the neural networks $\mu_\theta, \Sigma_\theta$) 를 "inverting" 하는 것과 대응됨.
  - $p(x  \vert  z) = N(\mu_\theta (z), \Sigma_\theta (z))$
- Interatable 한 $p(z \vert x; \theta)$ 에 최대한 가까운 분포를 근사하는 Variational parameter $\Phi$ 를 hidden variable로 가지는 **tractable한 $q(z; \Phi)$를** 이용해서 문제를 해결해보자
  - 예시, Gaussian with mean and covariance specfied by $\Phi$ $$q(z; \Phi)=N(\Phi_1, \Phi_2)$$
- **Variational inference**

  - pick $\Phi$ so that $q(z; \Phi)$ is as close as possible to $p(z \vert x;\theta)$
  - $p(z \vert x;\theta)$ 는 존재하는 건 알지만, 계산할 방법을 모르는 intractable optimal choice

- $log\,p(x;\theta)$ 를 다시 정리해보면,
  $log\,p(x;\theta) \geq \sum_z q(z;\Phi) log p(z,x;\theta) + H(q(z;\Phi)) = \mathcal{L}(x; \theta,\Phi)$
  $log\,p(x;\theta) = \mathcal{L}(x; \theta,\Phi) + D_{KL}(q(z;\Phi) \Vert p(z \vert x;\theta))$
- $q(z;\Phi)$를 posterior $p(z \vert x;\Theta)$ 에 더 잘 근사할 수록, KL-Divergence를 더 가깝게 할 수 있고, 이는 $log p(x;\theta)$를 더 잘 표현함을 의미.

- _다음은 데이터셋을 기반으로 $\theta$와 $\Phi$를 jointly 최적화하여, ELBO를 최대화 하는 것이 목표_

### The Evidence lower bound applied to the entire dataset

- 전체 데이터셋에 대하여 생각해보면,
  - 각 데이터 포인트 $x^i$ 별로, True Posterior $p(z \vert x^i;\theta)$ 는 달라지기 때문에,
  - 각 데이터 포인트 $x^i$ 별로 각기 다른 variational parameter $\Phi^i$를 가지는 것이 한 방법이 된다.
- 따라서, 
\begin{aligned}
\underset{\theta}{max}\,l(\theta; D) \geq \underset{\theta, \Phi^1,...,\Phi^M}{max} \sum\_{x^i \in D} \mathcal{L}(x^i; \theta, \Phi^i)
\end{aligned}

### Learning via stochastic variational inference (SVI)

- (stocahstic) gradient descent를 이용해서 위 식을 최적화 해보자.
- $\mathcal{L}(x^i; \theta, \Phi^i)=\sum_zq(z;\Phi^i) log\,p(z,x^i;\theta)+H(q(z;\Phi^i))$
  $= E_{q(z;\Phi^i)}[log\,p(z,x^i;\theta) - log\,q(z; \Phi^i)]$

1.  $\theta, \Phi^1,...,\Phi^M$ 을 초기화 한다.
2.  Dataset에서 랜덤하게 data point $x^i$를 sample한다.
3.  $\Phi^i$에 대하여 $\mathcal{L}(x^i; \theta, \Phi^i)$를 최적화 한다.
    1. Repeat $\Phi^i = \Phi^i + \eta \nabla_\Phi^i\mathcal{L}(x^i; \theta, \Phi^i)$
    2. until convergence to $\Phi^{i,*} \approx arg\,max_\Phi\mathcal{L}(x^i; \theta, \Phi^i)$
4.  $\theta$에 대해, $\nabla_\theta\mathcal{L}(x^i; \theta, \Phi^i)$ 를 계산한다.
5.  $\theta$를 그라디언트 방향으로 업데이트하고, step2로 돌아간다.

- 여기서 전체 z에 대해서 계산하기 어렵기 때문에, Monte Carlo sampling을 이용한다.
- 따라서,
\begin{aligned}
E_{q(z;\Phi^i)}[log\,p(z,x^i;\theta) - log\,q(z; \Phi^i)] \approx \frac{1}{K}\sum_klog\,p(z^k, x;\theta) - log\,q(z^k;\Phi^i)
\end{aligned}
- 이렇게 변경하면, 미분이 가능해지는데, $\theta$에 대해선 쉽게 미분 가능하지만, $\Phi$에 대해서는 미분하고 업데이트가 어렵다.
  - 샘플링에 $\Phi$가 영향을 주기 때문에, $\Phi$의 변화에 샘플링 프로세스가 어떻게 변하는지를 알아야 계산이 가능함. -> **Reparametrization**으로 해결해봅시다.

### Reparameterization

- _z가 continuous_ 할 때, 아래 식을 미분하려고 함. (r은 reward function, 앞선 Expection에 든 수식) 
\begin{aligned}
E_{q(z;\Phi)}[r(z)] = \int q(z;\Phi)r(z)dz
\end{aligned}
- $q(z;\Phi) = N(\mu,\sigma^2I)$ with parameter $\Phi = (\mu,\sigma)$로 가정하면 이를 2가지 방법으로 표현할 수 있다.
  - Sample $z \sim q(z; \Phi)$ : Directly sample
  - Sample $\epsilon \sim N(0, I), z = \mu + \sigma \epsilon = g(\epsilon; \Phi).$ $g$ is derministic. : 표준 정규 분포에 transform 하는 형태.
- 2번째 방식을 사용해보면,
  $E_{q(z;\Phi)}[r(z)] = \int q(z;\Phi)r(z)\,dz=E_{\epsilon\sim N(0, I)}[r(g(\epsilon;\Phi))]$ 이 되고,
  $\nabla_\Phi E_{q(z;\Phi)}[r(z)] = \nabla_\Phi E_{\epsilon\sim N(0, I)}[r(g(\epsilon;\Phi))] = E_{\epsilon\sim N(0, I)}[\nabla_\Phi(g(\epsilon;\Phi))]$
  으로 샘플링하는 부분을 정규 분포를 이용하게끔 변경하여, 최적화 가능하게 한다.
- 여기서 Monte Carlo를 적용해보면 아래와 같다. 
\begin{aligned}
E_{\epsilon\sim N(0, I)}[\nabla_\Phi(g(\epsilon;\Phi))] \approx \frac{1}{K} \sum_k \nabla_\Phi r(g(\epsilon^k;\theta))\,where\, \epsilon^1,...,\epsilon^K \sim N(0,I) 
\end{aligned}
- ELBO 식을 다시 써보면 아래와 같다.
\begin{aligned}
\mathcal{L}(x^i; \theta, \Phi^i)= E_{q(z;\Phi^i)}[\underbrace{log\,p(z,x^i;\theta) - log\,q(z; \Phi^i)}_{r(z,\Phi)}]
\end{aligned}
- 수식을 보면, Reward function이 $r(z)$가 아닌 $r(z, \Phi)$임을 알 수 있는데, 이 경우에도 chain rule을 이용하여, reparameterization이 가능하다.
\begin{aligned}
E_{q(z;\Phi)}[r(z,\Phi)] = E_\epsilon[r(g(\epsilon;\Phi),\Phi)] \approx \frac{1}{K}\sum_k r(g(\epsilon^k;\Phi), \Phi)
\end{aligned}

### Amortized Inference

- Monte carlo와 Reparameterization을 통해서 계산 가능하게 만들었으나, 각 데이터 포인트 마다 파라미터가 존재한다는 것이 문제가 된다. (데이터셋이 커지면, scalable 하지 않다.)
- 이를 **Amortization**을 통해 해결한다.
- **Amortization**
  - 각 데이터 포인트 x에 매핑되는 좋은 variational paramters를 추론하는 _single parametric function_ $f_\lambda$를 학습한다.
  - 예를 들어, $q(z \vert x^i)$가 각기 다른 평균을 가진 가우시안이라면,
    $x^i$가 $\mu^i$로 매핑하는 하나의 Neural network $f_\lambda$를 학습한다.
- 즉, posteriors $q(z \vert x^i)$를 $q_\lambda(z \vert x)$로 근사한다
- 정확하게 다시 써본다면, **Amortized inference**는 아래와 같다.
  - learn how to map $x^i$ to a good set of parameters $\Phi^i$ via $q(z; f_\lambda(x^i))$.
- 문헌에서는, $q(z; f_\lambda(x^i))$ 는 종종 $q_\Phi(z \vert x)$로 쓰이기도 함.

### learning with amortized inference

- Optimize $\sum_{x^i \in D}\mathcal{L}(x^i;\theta,\Phi)$ as a function of $\theta, \Phi$ using (stochastic) gradient descent
\begin{aligned}
\mathcal{L}(x^i;\theta,\Phi)=E_{q_\Phi(z \vert x)}[log\,p(z,x;\theta) - log\,q_\Phi(z \vert x)]
\end{aligned}

1. Initialize $\theta^{(0)}, \Phi^{(0)}$
2. Randomly sample a data point $x^i$ from $D$
3. Compute $\nabla_\theta\mathcal{L}(x^i;\theta,\Phi)$ and $\nabla_\Phi\mathcal{L}(x^i;\theta,\Phi)$ (reparametrization 적용)
4. Update $\theta, \Phi$ in the gradient direction

### Autoencoder perspective
\begin{eqnarray}
\mathcal{L}(x^i;\theta,\Phi) & = &E_{q_\Phi(z \vert x)}[log\,p(z,x;\theta) - log\,q_\Phi(z \vert x)]
\newline & = & E_{q_\Phi(z \vert x)}[log\,p(z,x;\theta) - log\,p(z) + log\,p(z) - log\,q_\Phi(z \vert x)]
\newline & = & E_{q_\Phi(z \vert x)}[log(p(x \vert z;\theta))] - D_{KL}(q_\Phi(z \vert x) \Vert p(z))
\end{eqnarray}

1. $q_\Phi(z \vert x^i)$ 의 샘플링으로, 데이터 포인트 $x^i$를 $\hat{z}$로 매핑 -> encoder
2. $p(x \vert \hat{z};\theta)$의 샘플링으로, $\hat{x}$를 복원. -> decoder

- 첫번째 텀은 $\hat{x} \approx x^i$ 를 하게끔 (reconstruction loss)
- 두번째 텀은 $\hat{z}$가 pior $p(z)$와 유사한 분포를 가지게 끔.

### 코드로 보는 VAE의 주요 부분

- FashionMNIST 데이터셋을 이용해 구현한 VAE full code : [link](https://github.com/joseph-jingi-jung/generative-dl-2nd/blob/main/vae.ipynb)
- Encoder 에서 z에 대한 평균과 분산, z를 만들어냄.
  - 여기서 평균과 분산은 reparametrization 전이므로, 정규분포에 근사해야함
    (뒤의 KL-divergnce loss 연결)

```python
class Encoder(nn.Module):
	...
	...
    def forward(self, x):
        out = self.conv1(x)
        out = self.activation(out)
        out = self.conv2(out)
        out = self.activation(out)
        out = self.conv3(out)
        out = self.activation(out)
        out = self.flatten(out)
		# 여러 CNN block을 거친 후, dense layer 를 거쳐서 z의 mu와 variance를 얻는 부분
        z_mean = self.dense_z_mean(out)
        z_log_var = self.dense_z_log_var(out)

	    # reparametrization을 이용한 Monte Carlo 샘플링
        z = self.sampling(z_mean, z_log_var)

        return z_mean, z_log_var, z
```

- Sampling
  - Monte Carlo Sampling (K=1)
    - batch와 z의 차원에 맞추어 정규분포 안에서 한 랜덤 값($\epsilon$)을 샘플링.
      (K=1이므로 한번만 샘플링)
    - reparametrization trick을 적용.

```python
    def forward(self, z_mean, z_log_var):
        batch = z_mean.shape[0]
        dim = z_mean.shape[1]
        epsilion = torch.randn([batch, dim])
        epsilion = epsilion.to(z_mean.device)
        return z_mean + torch.exp(0.5 * z_log_var) * epsilion
```

- Training Loss
  - reconstruction loss 는 디코더를 통해 생성된 $\hat{x}$와 데이터 포인트 $x$ 를 MSE 또는 CE를 통해서 loss를 계산
  - 정규분포와 $q(z \vert x)$ 간의 kl divergence 로 규제.

```python
			recon_loss = criterion(pred_compare, x_compare)
            kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())

            loss = recon_loss + kl_loss
	        ...
	        ...
            loss.backward()
```

### 정규분포와 $N(\mu, \sigma)$ 사이의 KL-Divergence

- KL Divergence의 정의
\begin{aligned}
D_{KL}​(q(z)∥p(z))=E_{q(x)}[log\frac{q(z)​}{p(z)}]
\end{aligned}
- 각 분포의 확률 밀도 함수
\begin{eqnarray}
q(z) &=& \frac{1}{\sqrt{2\pi\sigma^2}} exp(-\frac{(z-\mu)^2}{2\sigma^2}) \nonumber 
\newline p(z) &=& \frac{1}{\sqrt{2\pi}} exp(-\frac{z^2}{2}) \nonumber 
\end{eqnarray}
- KL Divergence에 대입하면,
\begin{eqnarray}
\nonumber D_{KL}​(q(z) \Vert p(z))&=&E_{q(z)}[log\frac{\frac{1}{\sqrt{2\pi\sigma^2}} exp(-\frac{(z-\mu)^2}{2\sigma^2})​}{\frac{1}{\sqrt{2\pi}} exp(-\frac{z^2}{2})}]
\nonumber \newline &=& E_{q(z)}[log(\frac{1}{\sqrt{2\pi\sigma^2}}) - \frac{(z-\mu)^2}{2\sigma^2} - log(\frac{1}{\sqrt{2\pi}}) + \frac{z^2}{2}]
\nonumber \newline &=& E_{q(z)}[log(\frac{1}{\sqrt{2\pi\sigma^2}}) + log(\sqrt{2\pi}) - \frac{(z-\mu)^2}{2\sigma^2}  + \frac{z^2}{2}]
\nonumber \newline &=& E_{q(z)}[-\frac{1}{2}log(2\pi\sigma^2) + \frac{1}{2}log(2\pi) - \frac{(z-\mu)^2}{2\sigma^2}  + \frac{z^2}{2}]
\nonumber \newline &=& E_{q(z)}[-\frac{1}{2}log(\sigma^2) - \frac{(z-\mu)^2}{2\sigma^2}  + \frac{z^2}{2}]
\nonumber \end{eqnarray}
  여기서 각각의 항에 대해 계산 한다면,
- 첫번째 항은 z와 무관한 상수이므로, 

$$
\begin{aligned}
E\_{q(z)}[-\frac{1}{2}log(\sigma^2) ] = -\frac{1}{2}log(\sigma^2)
\end{aligned}
$$

- 두번째 항에서 $(z-\mu)^2$의 기대 값은 $\sigma^2$ 이므로, 

$$
\begin{aligned}
E*{q(z)}[- \frac{(z-\mu)^2}{2\sigma^2}] = -\frac{1}{2\sigma^2} E*{q(z)}[(z-\mu)^2] = -\frac{1}{2}
\end{aligned}
$$

- 세번째 항은 z의 제곱에 대한 기대 값을 구하는 것 이므로, 

$$
\begin{aligned}
E_{q(z)}[\frac{z^2}{2}] = \frac{1}{2}E_{q(z)}[z^2] = \frac{1}{2}(\mu^2 +\sigma^2)
\end{aligned}
$$

- 따라서 전체를 결합하면,

$$
\begin{eqnarray}
D_{KL}​(q(z) \Vert p(z)) &=& -\frac{1}{2}log(\sigma^2) -\frac{1}{2} + \frac{1}{2}(\mu^2 +\sigma^2) \nonumber
\newline &=&\frac{1}{2}(\mu^2 + \sigma^2 - 1 - log(\sigma^2)) \nonumber
\end{eqnarray}
$$

  따라서 이는 아래의 코드와 동일하다,

```python
  kl_loss = 0.5 * torch.sum(z_mean.pow(2) + z_log_var.exp() - 1 - z_log_var)
```
