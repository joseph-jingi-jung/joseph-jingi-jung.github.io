---
layout: post
title: AI604 - CH6 CNN
subtitle: CNN
date: 2024-10-20 22:58:06 +0900
category: content
tags:
  - vision
use_math: true
---
AI604 수업을 수강 후 정리한 내용이다. Stanford의 CS231n과 맞닿아 있다.

### Components of a full-connected network
- Fully-Connected Layers
- Activation Function
- Convolution layers
- pooling layers
- Normalization
### Convolution Layer
- convolve : 합성곱하다
- Convolution :  filter를 이미지와 합성곱 하다 (i.e. 이미지를 공간적으로 슬라이드 하면서 내적(dot product)을 계산한다.)
- 3x32x32 image 와 3x5x5 filter
	- 필터와 image의 작은 청크(3x5x5) 사이의 dot product의 결과
		- $w^T x + b$
	- 모든 공간적 위치를 slide 하면서 activation map을 얻음
- 6개의 필터면, 6개의 activation map이 생성됨
#### Batch 연산
- $N \times C_{in} \times H \times W$ : 배치 이미지 (개,채,행,렬)
- $C_{out} \times C_{in} \times K_w \times K_h$ : 필터 (필터 갯수, 채널, 커널 행, 커널 렬)
- Conv 결과 : $N \times C_{out} \times H' \times W'$ : 배치 아웃풋
#### Conv filter는 무엇을 배우는가?
- 첫 번째 conv filter: local image templates (종종 엣지 방향, 대조색을 학습)
#### Padding
- Input W, Filter K, Padding P
- Output: W - K + 1 + 2P
	- 많은 경우 P = (K-1) / 2 로 두어, input과 동일한 크기로 맞춰줌
#### Receptive Fields
- 커널의 크기가 K 라면, 각 요소의 출력은 입력의 K x K receptive field 에 의존한다.
	- L layer가 추가 된다면, Receptive Field의 크기는 $1 + L*(K-1)$ 이된다.

$$
\begin{aligned}
R_2 &= R_1 + (K-1) *S
\\ R_3 &= R2 + (K-1) *S 
\\ &= R_1 + (K-1) *S + (K-1)*S
\\ R_L &= K + (K-1) * S * (L-1)
\\ &= 1 + (K-1) + (K-1)* S * (L-1)
\\ \text{If S=1, } R_L &= 1+ (K-1)*L
\end{aligned}
$$

#### Strided convolution
- Input W, Filter K, Padding P, Stride S
- Output: $(W - K + 2P) / S + 1$
#### Convolution Example
- Input : 3 x 32 x 32
- 10개의 5x5 filters
- stride 1, padding 2
- 출력 크기 : $(32 - 5 + 2*2)/1 + 1 = 32$ 따라서, 10 x 32 x 32
- 학습 파라미터 수 :
	- 필터 당 파라미터의 수 = $3 * 5 * 5 + 1\text{(bias)} = 76$
	- 10개의 필터이므로 총 파라미터 수 = $76*10 = 760$
- Multply-add 연산의 수 :
	- output의 수 : $10*32*32=10240$
	- 각 output 당 inner product의 수 $3*5*5=75$
	- 총 연산 수 = $75 * 10240 = 768K$
- 1x1 Convolution
	- 각 위치에 대한 MLP 연산과 같음
	- 채널 수 조정에 많이 쓰임
#### Pooling Layers
- downsampling 하는 다른 방식
- MaxPooling
	- 공간적 이동에 대한 불변성 (spatial shift invariance)
	- 작은 공간적 이동의 영향 감소
	- 각 윈도우의 가장 큰 값을 선택하기 때문에, 작은 이동으로 인해 해당 특징의 위치가 약간 변하더라도 영역 내에서 가장 큰 값은 여전히 비슷한 위치에 선택 될 가능성이 높기 때문.
- 별도의 학습 파라미터 없음
### Normalization
- Deep network는 학습하기 어려움!
#### Batch Normalization
- 레이어의 출력을 정규화하여 평균이 0이고 분산이 1이 되도록 한다.
- Covariate shift
	- 입력 데이터의 분포가 변하는 것.
- Internal Covariate shift
	- 각 레이어로 들어오는 입력 분포가 계속 변화하는 현상
- 따라서, Internal Covariate shift가 심해지면, 각 레이어가 계속 변화하는 입력 분포에 맞추어 학습해야 하기 때문에 학습이 느려지고 불안정 해짐
- Normalization이 그 문제를 줄여줌

$$
\hat{x}^{(k)} = \frac {x^{(k)} - E[x^{(k)}]}{\sqrt{VAR[x^{(k)}]}}
$$

- 미분 가능한 함수라서, backprop에 사용 할 수 있음
##### 1 차원 Batch Normalization
- 입력 : $x : N \times D$
- $\mu_j = \frac 1 N \sum^N_{i=1} x_{i,j}$  채널 별 평균. shape 은 D
- $\sigma^2_j = \frac 1 N \sum^N_{i=1}(x_{i,j} - \mu_j)^2$ 채널 별 표준편차. shape은 D
- $\hat{x}_{i,j}=\frac{x_{i,j} - \mu_j}{\sqrt{\sigma^2_j + \epsilon}}$ 정규화 된 x,  Shape는 N x D
- Problem: 만약 평균 0, 분산 1이 지나치게 엄격한 제약이라면?
	- 정규화된 결과에 학습 할 수 있는 scale과 shift를 추가해주자!
	- $y_{i,j} = \gamma_j \hat{x}_{i,j} + \beta_j$ 정규화된 결과에 scale과 shift 추가. shape는 NxD
- Test time
	- 학습 할 때 측정한 $\mu_j, \sigma^2_j$ 의 이동 평균, 이동 분산 값을 모델의 파라미터로 저장해두었다가 test-time에 사용함.
	- $\gamma, \beta$ 또한 저장해둔 값을 사용.
#### ConvNet의 Batch Normalization

$$
\begin{aligned}
x &: N \times C \times H \times W
\\ &\text{Normalize}
\\ \mu, \sigma &: 1 \times C \times 1 \times 1
\\ \gamma, \beta &: 1 \times C \times 1 \times 1
\\ y &= \gamma \frac{(x - \mu)}{\sigma} + \beta
\end{aligned}
$$

- 보통 FC layer 또는 conv layer 뒤, activation 전에 넣음
- 장점
	- 학습을 좀 더 쉽게 만듬
	- 좀 더 높은 learning rate와 빠른 수렴을 도움
	- 네트워크 초기화에 덜 민감해짐
	- 학습 도중 정규화와 같은 행동
	- 테스트 타임에 오버헤드가 없음. (conv에 합칠 수 있음)
- 단점
	- 아직 완벽하게 이론적으로 설명되지 않음
	- 학습과 테스트 타임의 동작이 다름
#### Layer normalization

$$
\begin{aligned}
x &: N \times C \times H \times W 
\\ &\text{Normalize over all dimensions except N (batch dimension)} 
\\ \mu, \sigma &: N \times 1 \times 1 \times 1 \\ \gamma, \beta &: 1 \times C \times 1 \times 1 
\\ &(\text{or match input shape, based on implementation}) 
\\ y &= \gamma \frac{(x - \mu)}{\sigma} + \beta
\end{aligned}
$$

- train과 test에 동일하게 동작 
	- 배치에 걸친 통계 아니라 개별 샘플 내부 특성에 대한 평균과 테스트 이므로
#### Instance Normalization

$$
\begin{aligned}
x &: N \times C \times H \times W
\\ &\text{Instance normalize}
\\ \mu, \sigma &: N \times C \times 1 \times 1
\\ \gamma, \beta &: 1 \times C \times 1 \times 1
\\ y &= \gamma \frac{(x - \mu)}{\sigma} + \beta
\end{aligned}
$$

#### Group Normalization

$$
\begin{aligned}
x &: N \times C \times H \times W 
\\ G &: \text{number of groups} 
\\ \mu_g, \sigma_g &: \text{mean and variance for each group} 
\\ &: N \times G \times 1 \times 1
\\ \gamma, \beta &: \text{learnable scale and shift (per channel)} 
\\ &: 1 \times C \times 1 \times1
\\ y &= \gamma \frac{(x - \mu_g)}{\sigma_g} + \beta
\end{aligned}
$$
- 채널을 그룹으로 나누어 Layer Norm
- Group의 개수는 하이퍼파라미터
- 그룹별로 계산된 통계를 사용하여서, 좀 더 작고 세분화된 범위를 커버(Layer Norm 대비)
- G가 1이면, Layer Norm, C 이면 Instance Norm
- 배치 크기에 의존하지 않고, 소규모 데이터셋에 적합. 공간적 구조를 가진 데이터에도 적합.

